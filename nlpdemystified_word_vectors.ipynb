{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuPdnyWXh7vb"
      },
      "source": [
        "# Natural Language Processing Demystified | Word Vectors\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/futuremojo/nlp-demystified<br><br>\n",
        "Course module for this demo: https://www.nlpdemystified.org/course/word-vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91LZr9ogtHi_"
      },
      "source": [
        "**IMPORTANT**<br>\n",
        "Enable **GPU acceleration** by going to *Runtime > Change Runtime Type*. Keep in mind that, on certain tiers, you're not guaranteed GPU access depending on usage history and current load.\n",
        "<br><br>\n",
        "Also, if you're running this in the cloud rather than a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity.\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOE58pxDZqah"
      },
      "source": [
        "To demonstrate word vectors, we're going to use **Gensim** which we first encountered in the [topic modelling](https://www.nlpdemystified.org/course/topic-modelling) module.<br>\n",
        "\n",
        "At the time of this recording, the default Gensim version in Colab was 3.X, so we'll first upgrade to 4.X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2vNBSTzIKkRT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim==4.*\n",
            "  Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim==4.*) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim==4.*) (1.11.2)\n",
            "Collecting smart-open>=1.8.1 (from gensim==4.*)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl (24.0 MB)\n",
            "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/24.0 MB 1.3 MB/s eta 0:00:19\n",
            "   ---------------------------------------- 0.2/24.0 MB 2.6 MB/s eta 0:00:10\n",
            "    --------------------------------------- 0.5/24.0 MB 4.6 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 0.9/24.0 MB 5.5 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 1.1/24.0 MB 5.1 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 1.2/24.0 MB 4.9 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 1.2/24.0 MB 4.6 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 1.2/24.0 MB 4.6 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 1.5/24.0 MB 3.9 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 1.8/24.0 MB 4.1 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 1.9/24.0 MB 4.2 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 2.1/24.0 MB 4.2 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 2.3/24.0 MB 4.1 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 2.3/24.0 MB 4.1 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 2.3/24.0 MB 4.1 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 2.6/24.0 MB 3.7 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 2.9/24.0 MB 4.1 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 3.1/24.0 MB 4.1 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 3.3/24.0 MB 4.0 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 3.4/24.0 MB 4.0 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 3.4/24.0 MB 4.0 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 3.4/24.0 MB 4.0 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 3.7/24.0 MB 3.7 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 4.2/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 4.3/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 4.5/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 4.7/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 4.9/24.0 MB 3.9 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 5.0/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 5.2/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 5.4/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 5.6/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 5.7/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 5.9/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 6.1/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 6.3/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 6.4/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 6.6/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 6.8/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 6.9/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 7.1/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 7.3/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 7.5/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 7.6/24.0 MB 3.9 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 7.8/24.0 MB 3.9 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 8.0/24.0 MB 3.9 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 8.2/24.0 MB 4.0 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 8.4/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 8.5/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 8.7/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 8.9/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 9.1/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 9.2/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 9.4/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 9.6/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 9.8/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 9.9/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 10.1/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 10.3/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 10.5/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 10.7/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 10.9/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 11.1/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 11.2/24.0 MB 3.8 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 11.4/24.0 MB 3.8 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 11.6/24.0 MB 4.0 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 11.7/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 11.9/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 12.1/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 12.2/24.0 MB 3.9 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 12.4/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 12.6/24.0 MB 4.0 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 12.8/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 12.9/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 13.1/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 13.3/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 13.5/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 13.7/24.0 MB 4.0 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 13.8/24.0 MB 4.0 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 14.0/24.0 MB 4.0 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 14.2/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 14.3/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 14.5/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 14.7/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 14.9/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 15.0/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 15.2/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 15.4/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 15.5/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 15.7/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 15.9/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 16.1/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 16.2/24.0 MB 3.9 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 16.4/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 16.6/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 16.7/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 16.9/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 17.1/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 17.2/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 17.4/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 17.6/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 17.8/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 18.0/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 18.1/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 18.2/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 18.4/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 18.6/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 18.7/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 18.9/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 19.1/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 19.3/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 19.4/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 19.6/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 19.8/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 19.9/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 20.1/24.0 MB 3.9 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 20.3/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 20.5/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 20.6/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 20.8/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 21.0/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 21.1/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 21.3/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 21.5/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 21.6/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 21.8/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 22.0/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 22.1/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 22.3/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 22.5/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 22.7/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 22.8/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 23.0/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 23.1/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 23.3/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  23.5/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  23.7/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  23.8/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 24.0/24.0 MB 3.8 MB/s eta 0:00:00\n",
            "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 57.0/57.0 kB 3.1 MB/s eta 0:00:00\n",
            "Installing collected packages: smart-open, gensim\n",
            "Successfully installed gensim-4.3.2 smart-open-6.4.0\n",
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Upgrade gensim just in case.\n",
        "%pip install -U gensim==4.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.1.3-cp310-cp310-win_amd64.whl (10.7 MB)\n",
            "     ---------------------------------------- 10.7/10.7 MB 3.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (1.26.0)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "     -------------------------------------- 502.5/502.5 kB 7.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
            "Collecting tzdata>=2022.1\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "     -------------------------------------- 341.8/341.8 kB 7.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.1.3 pytz-2023.3.post1 tzdata-2023.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-win_amd64.whl (12.1 MB)\n",
            "     ---------------------------------------- 12.1/12.1 MB 3.8 MB/s eta 0:00:00\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.26.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "     ---------------------------------------- 181.6/181.6 kB ? eta 0:00:00\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
            "     -------------------------------------- 122.2/122.2 kB 7.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Collecting typer<0.10.0,>=0.3.0\n",
            "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (23.2)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
            "  Downloading pydantic-2.5.1-py3-none-any.whl (381 kB)\n",
            "     -------------------------------------- 381.6/381.6 kB 7.9 MB/s eta 0:00:00\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Collecting thinc<8.3.0,>=8.1.8\n",
            "  Downloading thinc-8.2.1-cp310-cp310-win_amd64.whl (1.5 MB)\n",
            "     ---------------------------------------- 1.5/1.5 MB 4.5 MB/s eta 0:00:00\n",
            "Collecting jinja2\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "     ---------------------------------------- 50.1/50.1 kB 2.7 MB/s eta 0:00:00\n",
            "Collecting wasabi<1.2.0,>=0.9.1\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
            "     ------------------------------------- 481.9/481.9 kB 10.0 MB/s eta 0:00:00\n",
            "Collecting tqdm<5.0.0,>=4.38.0\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
            "Collecting pydantic-core==2.14.3\n",
            "  Downloading pydantic_core-2.14.3-cp310-none-win_amd64.whl (1.9 MB)\n",
            "     ---------------------------------------- 1.9/1.9 MB 4.6 MB/s eta 0:00:00\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
            "     ---------------------------------------- 6.6/6.6 MB 3.5 MB/s eta 0:00:00\n",
            "Collecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "     ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->spacy) (2.1.3)\n",
            "Installing collected packages: cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, langcodes, jinja2, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
            "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.3 cymem-2.0.8 jinja2-3.1.2 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.5.1 pydantic-core-2.14.3 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 tqdm-4.66.1 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\MSHOME\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script weasel.exe is installed in 'c:\\Users\\MSHOME\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script spacy.exe is installed in 'c:\\Users\\MSHOME\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas\n",
        "%pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-win_amd64.whl (9.3 MB)\n",
            "     ---------------------------------------- 9.3/9.3 MB 3.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.26.0)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 threadpoolctl-3.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (2.14.0)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: tensorflow-intel==2.14.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (63.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.24.4)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.5.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.59.0)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.5)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.23.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.2.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn\n",
        "%pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kIMAMpBaoqkP"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBQlbyiVY3WJ"
      },
      "source": [
        "**NOTE**<br>\n",
        "In this notebook, we won't train standalone word embeddings from scratch. Rather, we'll:\n",
        "1. Use *pretrained* embeddings in one model.\n",
        "2. Train embeddings alongside another model.\n",
        "<br>\n",
        "\n",
        "If you want to try training standalone word embeddings, coding Skip-Gram With Negative Sampling (SGNS) from scratch shouldn't be too hard now that you know all the details. But I recommend just using the **Gensim** library instead:<br>\n",
        "https://radimrehurek.com/gensim/models/word2vec.html<br>\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_rgKP1vJ2kT"
      },
      "source": [
        "# Using Pretrained, Third-Party Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHMDs5FPKXUx"
      },
      "source": [
        "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQEAXBoBMidI"
      },
      "source": [
        "We'll need to first download the actual word vectors. It's over a gigabyte in size but will fit within our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dIKSA-KMbj8E"
      },
      "outputs": [],
      "source": [
        "\n",
        "!gdown \"https://drive.google.com/uc?id=1BpfbHu4denceXiv8yfdY3EHgjKIcULku\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FtVSlPQbj4F"
      },
      "outputs": [],
      "source": [
        "embedding_file = './GoogleNews-vectors-negative300.bin.gz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXQRngPaPWhE"
      },
      "source": [
        "Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "<br><br>\n",
        "To save time and space, we'll limit ourselves to 200,000 word vectors for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmRUiKPObjqM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir22T0t3QqPS"
      },
      "source": [
        "Retrieving a word's vector is a matter of using a token as a key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm9lDs3Vfv12"
      },
      "outputs": [],
      "source": [
        "pizza = word_vectors['pizza']\n",
        "print(f'Vector dimension: {pizza.shape}')\n",
        "\n",
        "# The embedding for the word 'pizza'.\n",
        "print(pizza)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORLN9U9fq8hp"
      },
      "source": [
        "We can get the cosine similarity between two words using the *similarity* method.\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-7Iuo2orp60"
      },
      "source": [
        "In this case, we see words which we expect to be related have a higher similarity measure..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1i6H88Cfp1P"
      },
      "outputs": [],
      "source": [
        "print(word_vectors.similarity('pizza', 'tomato'))\n",
        "print(word_vectors.similarity('pizza', 'sauce'))\n",
        "print(word_vectors.similarity('pizza', 'cheese'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UWdQJzaryQL"
      },
      "source": [
        "...compared to words we don't expect to be related."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUXR8uQ0ftWF"
      },
      "outputs": [],
      "source": [
        "print(word_vectors.similarity('pizza', 'gorilla'))\n",
        "print(word_vectors.similarity('pizza', 'tree'))\n",
        "print(word_vectors.similarity('pizza', 'yoga'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3cD4P1tr4fT"
      },
      "source": [
        "Out-of-vocabulary (OOV) words thrown an exception. Bear in mind that looking up even common words here can result in an exception because we loaded only a subset of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLf9bpYihVz7"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  word_vectors['womblyboo']\n",
        "except KeyError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCQcU72-q0V5"
      },
      "source": [
        "We can compare two sentences by using *n_similarity* which computes the cosine similarity of two **sets** of words.\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.n_similarity<br><br>\n",
        "*n_similarity* expects a sentence as a list of words, hence the use of '.split()'. It'll take this list of words, calculate the average of its word vectors, and use the result as an embedding for the whole sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwvDT25miXk0"
      },
      "outputs": [],
      "source": [
        "word_vectors.n_similarity(\"dog bites man\".split(), \"canine nips human\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rgATfqBisrn"
      },
      "outputs": [],
      "source": [
        "word_vectors.n_similarity(\"martian dolphins are hostile\".split(), \"i flunked calligraphy school\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZLcvAU4vf6P"
      },
      "source": [
        "One downside of this approach is that word order is thrown out, so two sentences with identical words which mean different things would score a perfect similarity score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yikNOChTinp0"
      },
      "outputs": [],
      "source": [
        "word_vectors.n_similarity(\"dog bites man\".split(), \"man bites dog\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLLpgr06vxTQ"
      },
      "source": [
        "Doing quick-and-dirty similarity measures like this is probably best if your corpus is domain-specific and similarity is based more on keywords. The more specific, the better.<br><br>\n",
        "For example, a corpus of business news headlines would probably work well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EejHDg24iyum"
      },
      "outputs": [],
      "source": [
        "s1 = \"Volkswagen intends to double electric car sales in China\".lower().split()\n",
        "s2 = \"First Toyota with solid state battery will be hybrid\".lower().split()\n",
        "word_vectors.n_similarity(s1, s2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWsFWJUXQ9RR"
      },
      "source": [
        "The *most_similar* method returns the words with the closest vectors.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f2CXSQNxZgf"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar(positive=['cell'], topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YN45ETCReXK"
      },
      "source": [
        "We can also combine vectors first, and retrieve the words most similar to their mean. Here, we're combining the vectors for 'cell' and 'phone' and retrieving the vectors closest to that result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvOkR7UWx1su"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar(positive=['cell', 'phone'], topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjMdhqJvR7ez"
      },
      "source": [
        "Given a collection of words, the *doesn't_match* method returns the word that \"doesn't go\" with the rest (i.e. with the vector that's furthest away from the mean of all the other vectors).<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.doesnt_match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA2NVKHPzExZ"
      },
      "outputs": [],
      "source": [
        "word_vectors.doesnt_match([\"apple\", \"orange\", \"hamburger\", \"banana\", \"kiwi\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xajz9UFQR91b"
      },
      "source": [
        "We can see the power of context in this example with 'Toyota' being correctly identified as the odd one out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evIAtmcVygyn"
      },
      "outputs": [],
      "source": [
        "word_vectors.doesnt_match([\"Microsoft\", \"Apple\", \"Toyota\", \"Amazon\", \"Netflix\", \"Google\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6UyREbVUS6p"
      },
      "source": [
        "Visualizing word vectors is straight-forward and can offer insights into what kind of contexts the training algorithm picked up.<br><br>\n",
        "Because these word vectors have a dimension of 300, we need to reduce them down to two dimensions to plot them on a regular graph. This can be done through **Principal Components Analysis (PCA)**:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html<br>\n",
        "<br>\n",
        "Here, we're plotting the words we considered in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC1e7EF0-b-s"
      },
      "outputs": [],
      "source": [
        "def display_pca_scatterplot(model, words):        \n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r', s=128)\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqmNesyaEhq_"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(word_vectors, ['swim', 'swimming', 'cat', 'dog', 'feline', 'road', 'car', 'bus'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXE5AEEUSnEN"
      },
      "source": [
        "We can even solve analogies (to a limited extent) with vector arithmetic.<br><br>\n",
        "Here, we're solving the analogy:<br>\n",
        "_Rome is to Italy as London is to __________.<br><br>\n",
        "Arithmetically, this is Italy + London - Rome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCJZYi-1zNUI"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar(positive=['Italy', 'London'], negative=['Rome'], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vunBH2R5XRWe"
      },
      "source": [
        "Visualizing it can help with geometric intuition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlH9W8q2V0Dm"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(word_vectors, ['Rome', 'Italy', 'London', 'Britain', 'UK'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuDWv6NXXjoF"
      },
      "source": [
        "# Using Pretrained Word Vectors for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am-9bvppYKuS"
      },
      "source": [
        "In this section, we'll train a **Keras** model to use these Google News vectors to perform sentiment analysis on a bunch of **Yelp** reviews.\n",
        "<br><br>\n",
        "For this model, we'll increase the number of word vectors loaded to 1,000,000.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVLj4fSMzvHJ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=1000000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdZQNavVYxfP"
      },
      "source": [
        "The dataset we'll use is *Yelp Polarity Reviews*, a collection of ~600,000 reviews for both training and testing.<br><br>\n",
        "The original Yelp reviews use a five-star rating system. The ratings in this dataset have been modified to simply be negative (label==1) or positive (label==2).<br>\n",
        "https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews<br><br>\n",
        "Tensorflow comes with a datasets loader but we're going to download the file manually and process the data ourselves for completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcMEE11bzz_j"
      },
      "outputs": [],
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTnApF4aq7k"
      },
      "source": [
        "Unzipping the archive results in *train.csv* and *test.csv* files placed in the default *contents* folder of our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UK1y9kjzz7y"
      },
      "outputs": [],
      "source": [
        "!tar xvzf /root/input/yelp_review_polarity_csv.tgz\n",
        "\n",
        "# Show current working directory.\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaJCSM3lbK-E"
      },
      "source": [
        "The **Pandas** library makes it simple to load a CSV file into memory and manipulate the data.<br>\n",
        "https://pandas.pydata.org/<br>\n",
        "https://pandas.pydata.org/docs/<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIG4UVMkbq9E"
      },
      "source": [
        "Here, we're loading the CSV into a Pandas **dataframe** (sort of like an in-memory table) and explicitly naming the columns.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdLoK1Jozz0Z"
      },
      "outputs": [],
      "source": [
        "yelp_train = pd.read_csv('yelp_review_polarity_csv/train.csv', names=['sentiment', 'review'])\n",
        "print(yelp_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGgJl1QCb738"
      },
      "source": [
        "We can get a quick view of the data through the *head* method.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVHtVqOBzzwS"
      },
      "outputs": [],
      "source": [
        "yelp_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wS0o4gEclCT"
      },
      "source": [
        "To save on training time, we'll train on 100,000 reviews rather than the full set. To do that, we'll shuffle the dataset using the *sample* method and *copy* the first 100,000 entries. The reason to shuffle first is to ensure we get a mix of reviews from a variety of businesses (in case the data is sorted in some way).<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YacSH4F0c5aU"
      },
      "outputs": [],
      "source": [
        "TRAIN_SIZE = 100000\n",
        "yelp_train = yelp_train.sample(frac=1, random_state=1)[:TRAIN_SIZE].copy()\n",
        "print(yelp_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLraxnTMd9iE"
      },
      "source": [
        "The next thing to do is adjust the labels. This is a **binary classification problem**, so our model's output layer will be a single unit with a **sigmoid** activation function. This function's output will be between 0 and 1 which is then compared against the training label. But the labels are currently 1 for negative, and 2 for positive, which is going to cause problems when calculating the loss.<br><br>\n",
        "So we'll simply replace the 1s with 0s, and 2s with 1s using the *replace* method.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
        "<br><br>\n",
        "Alternatively, we could keep the labels as-is and treat this as a **multiclassification** problem with two labels and use a **softmax**, but we would then need to **one-hot encode** the labels (at least based on what we've learnt so far).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbmYKm-SjGnm"
      },
      "outputs": [],
      "source": [
        "yelp_train['sentiment'].replace(to_replace=1, value=0, inplace=True)\n",
        "yelp_train['sentiment'].replace(to_replace=2, value=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlzWLECijrg_"
      },
      "outputs": [],
      "source": [
        "yelp_train.head() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rILnYdQMgIlh"
      },
      "source": [
        "As we've done throughout this course, we'll create train/validation splits.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC-VSNcn7Cpq"
      },
      "outputs": [],
      "source": [
        "yelp_train_split, yelp_val_split = train_test_split(yelp_train, train_size=0.85, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xj4WyLb7Wdr"
      },
      "outputs": [],
      "source": [
        "# Set up training data.\n",
        "train_reviews = yelp_train_split['review']\n",
        "y_train = np.array(yelp_train_split['sentiment'])\n",
        "\n",
        "# Set up validation data.\n",
        "val_reviews = yelp_val_split['review']\n",
        "y_val = np.array(yelp_val_split['sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9V0dJAgbSJ"
      },
      "source": [
        "A quick sanity check to see how our data is distributed (e.g. balanced or skewed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHBWB2vc7ie1"
      },
      "outputs": [],
      "source": [
        "collections.Counter(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0fM5vRygqfq"
      },
      "source": [
        "Because we're relying more on richer encodings (in this case, word vectors), we won't perform as much preprocessing this time around. We'll stick with using the regular Keras **tokenizer** and just filter out numbers and certain symbols.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer<br><br>\n",
        "We'll also have the tokenizer limit itself to tokenizing only the most frequent 20,000 words. This way, the model will focus on the most frequent descriptive sentiment words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI4FgHPa4p2T"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=20000,\n",
        "                                               filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
        "                                               lower=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The active selection is creating a `Tokenizer` object from the `keras.preprocessing.text` module. This `Tokenizer` is a crucial part of text preprocessing in Natural Language Processing (NLP) tasks. It helps to convert the text data into numerical data, which can be fed into machine learning models.\n",
        "\n",
        "Here's a breakdown of the parameters used in the `Tokenizer`:\n",
        "\n",
        "- `num_words=20000`: This parameter sets the maximum number of words to be kept based on the frequency of words. In this case, only the top 20000 most frequent words will be considered, and less frequent words will be discarded.\n",
        "\n",
        "- `filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'`: This string contains all the characters that will be removed from the texts during the tokenization process. Here, it includes digits, punctuation, and special characters. This is useful for simplifying the text data and focusing on the words.\n",
        "\n",
        "- `lower=True`: This parameter, when set to True, converts all the text into lowercase. This is done to ensure that the same words in different cases are not considered as different words. For example, 'Hello' and 'hello' will be treated as the same word.\n",
        "\n",
        "After this `Tokenizer` is created, it can be used to fit on the text data and transform it into sequences of integers with the `fit_on_texts` and `texts_to_sequences` methods, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXQXX82chivF"
      },
      "source": [
        "Build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayGLszCqzzoK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "tokenizer.fit_on_texts(train_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The active selection is using the `fit_on_texts` method of the `Tokenizer` object to fit the tokenizer on the training reviews. Here's a breakdown:\n",
        "\n",
        "- `%%time`: This is a magic command in Jupyter notebooks that times the execution of a specific cell.\n",
        "\n",
        "- `tokenizer.fit_on_texts(train_reviews)`: This line is doing the main work.\n",
        "\n",
        "  - `fit_on_texts`: This is a method of the `Tokenizer` class that updates the internal vocabulary based on a list of texts. This method should be used before `texts_to_sequences` or `texts_to_matrix` to ensure the tokenizer has been fitted on the texts.\n",
        "\n",
        "  - `train_reviews`: This is the list of texts that the tokenizer is being fitted on. In this case, it's the training reviews.\n",
        "\n",
        "After running this cell, the tokenizer will have been fitted on the training reviews, and you can use it to convert these reviews (or other texts) to sequences or matrices.\n",
        "\n",
        "Next steps:\n",
        "- Convert the training reviews to sequences using `tokenizer.texts_to_sequences(train_reviews)`.\n",
        "- Check the word index of the tokenizer using `tokenizer.word_index` to see the mapping of words to integers.\n",
        "- Try fitting the tokenizer on different texts and see how it affects the word index and the sequences/matrices it generates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5zDD6RCBzxH"
      },
      "source": [
        "The next step is to vectorize our reviews. In the [_Neural Network Foundations_](https://github.com/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_neural_networks_foundations.ipynb) notebook, we used the *texts_to_matrix* method to turn text into binary bags of words.<br><br>\n",
        "Here, we're going to use the *text_to_sequences* method to turn each review into a sequence of integers, with each integer representing its corresponding token.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OAd431jp6Ydg"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "X_train = tokenizer.texts_to_sequences(train_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPo0mNRB6YZL"
      },
      "outputs": [],
      "source": [
        "# The first review in the training set, vectorized.\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH4I4SeoqVDn"
      },
      "source": [
        "We can look up the corresponding tokens using the tokenizer's *index_word* dict. Here are the tokens corresponding to the first three integers from the first vectorized review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkOI7jYFqb44"
      },
      "outputs": [],
      "source": [
        "[tokenizer.index_word[x] for x in X_train[0][:3]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahgfyZhDrHDJ"
      },
      "source": [
        "We can also convert the integer sequence back to text using the *sequences_to_texts* method, and compare it against the original text.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPlvLdz8rGZZ"
      },
      "outputs": [],
      "source": [
        "# Review excerpt reconstructed from integer sequence.\n",
        "tokenizer.sequences_to_texts([X_train[0]])[0][:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZirHR4_OC87H"
      },
      "outputs": [],
      "source": [
        "# Original review text.\n",
        "train_reviews.iloc[0][:300]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEqDNjactNk1"
      },
      "source": [
        "Some models and situations require us to **pad** our sequences to the same length. While that's not the case here, it can still be beneficial to have all our inputs (and consequently, our batches) to be of uniform size to help with optimizations.<br><br>\n",
        "In this case, we'll make all our reviews 200 tokens in length (in practice, you can choose a number based on some analysis). So the reviews longer than 200 tokens will be truncated, while the reviews shorter than 200 will be padded with zeroes.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lTEjMgDqIPp"
      },
      "outputs": [],
      "source": [
        "MAX_REVIEW_LEN = 200\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2tHPfAVv37k"
      },
      "outputs": [],
      "source": [
        "print(X_train[0])\n",
        "print(X_train[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBAg3laKwDWO"
      },
      "source": [
        "Our training set is prepared. We can now also vectorize and pad our validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9P72JJYHqKp"
      },
      "outputs": [],
      "source": [
        "X_val = tokenizer.texts_to_sequences(val_reviews)\n",
        "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=MAX_REVIEW_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8dy9OWMwafM"
      },
      "source": [
        "Now we need to incorporate the Google News vectors (currently loaded into gensim) into our Keras model. What we'll do is create an embedding matrix that maps each tokenizer integer to its respective word vector.<br><br>\n",
        "For example, here's the index for the word \"good\" from the Keras tokenizer and the word vector for \"good\" from gensim. We want a matrix which maps the index to the vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m7OW6-pwaJc"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.word_index['good'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rOAy_IkHqfx"
      },
      "outputs": [],
      "source": [
        "# Part of the vector for the word 'good'.\n",
        "print(word_vectors['good'][:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elzisAtxyO-o"
      },
      "source": [
        "We'll create this embedding matrix by first initializing a matrix of zeros, then looping over every word in the tokenizer vocabulary and:\n",
        "1. Checking if the word has a corresponding vector in gensim.\n",
        "2. If it does, then copy the vector into the matrix row corresponding to the word's index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw25Irs4J-JG"
      },
      "outputs": [],
      "source": [
        "# + 1 to account for padding token.\n",
        "num_tokens = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Initialize a matrix of zeroes of size: vocabulary x embedding dimension.\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word_vectors.has_index_for(word):\n",
        "    embedding_matrix[i] = word_vectors[word].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uncvu4lcJ-OI"
      },
      "outputs": [],
      "source": [
        "# Quick visual check.\n",
        "print(embedding_matrix[tokenizer.word_index['good']][:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmzNcyFu0Noo"
      },
      "source": [
        "We're ready to build our first model using pretrained word vectors. The first layer we'll add is a Keras **embedding** layer which is essentially a trainable lookup table/matrix.<br>\n",
        "https://keras.io/api/layers/base_layer/#layer-class<br>\n",
        "https://keras.io/api/layers/core_layers/embedding/<br><br>\n",
        "In this case, we'll populate the **embedding** layer with the embedding matrix we created, and set *trainable* to True. This means we'll allow the learning algorithm training the classification model to adjust/fine-tune the word vectors as needed for greater performance. This corresponds to one of the scenarios we covered in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALs6rkTnJ-aj"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    input_length=MAX_REVIEW_LEN,\n",
        "    trainable=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqRgEWpb2iuI"
      },
      "source": [
        "We'll use a simple architecture for this model. Each training example is a sequence of *integers* which gets converted to a sequence of *vectors* (embeddings), but subsequent layers are expecting one vector per review. So we're inserting a **GlobalAveragePooling1D** layer after the embedding layer to average out all the word vectors into a single vector, before sending it further into the network. For classification, this can be pretty effective as a base model approach.<br>\n",
        "https://keras.io/api/layers/pooling_layers/global_average_pooling1d/<br>\n",
        "\n",
        "There was no science behind choosing 128 units in the first hidden layer and 64 units in the second hidden layer. The intuition was that the signal would be distilled from 300 dimensions down to 128 dimensions, then down to 64 dimensions before going to output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RrLd4gzbhVS"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# This layer will output a sequence of 300-dimension *vectors*, one for each element in the input sequence.\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# This layer will calculate an average of those vectors.\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlmis9Oz4_jy"
      },
      "source": [
        "Here's an example of what's going to happen under the hood to turn review text into a single vector for the dense layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r5K-Qqb1d-H"
      },
      "outputs": [],
      "source": [
        "review = \"fantastic papaya steak\"\n",
        "print(f\"Review: {review}\")\n",
        "\n",
        "review_sequence = tokenizer.texts_to_sequences([review])\n",
        "print(f\"Review as sequence of integers: {review_sequence}\")\n",
        "\n",
        "review_embeddings = embedding_layer(np.array(review_sequence))\n",
        "print(f\"Review embeddings shape: (Batch size: {review_embeddings.shape[0]}, \\\n",
        "Sequence length: {review_embeddings.shape[1]}, \\\n",
        "Embedding size: {review_embeddings.shape[2]})\")\n",
        "\n",
        "# How our document will be presented to the rest of the neural network.\n",
        "print(f\"Average of embeddings (shape): {np.mean(review_embeddings, axis=1).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546ouDEv41Ea"
      },
      "source": [
        "When we call the model's *summary* method, note how there are no params for the **GlobalAveragePooling1D** layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz31QQcb5nq5"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwGYMD028zhO"
      },
      "source": [
        "We won't use **early stopping** for this run. This way, we'll be able to compare metrics between the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU6QBey79cis"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Neard5tEGwnG"
      },
      "outputs": [],
      "source": [
        "def plot_train_vs_val_performance(history):\n",
        "  training_losses = history.history['loss']\n",
        "  validation_losses = history.history['val_loss']\n",
        "\n",
        "  training_accuracy = history.history['accuracy']\n",
        "  validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(1, len(training_losses) + 1)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  fig, (ax1, ax2) = plt.subplots(2)\n",
        "  fig.set_figheight(15)\n",
        "  fig.set_figwidth(15)\n",
        "  fig.tight_layout(pad=5.0)\n",
        "\n",
        "  # Plot training vs. validation loss.\n",
        "  ax1.plot(epochs, training_losses, 'bo', label='Training Loss')\n",
        "  ax1.plot(epochs, validation_losses, 'b', label='Validation Loss')\n",
        "  ax1.title.set_text('Training vs. Validation Loss')\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.legend()\n",
        "\n",
        "  # PLot training vs. validation accuracy.\n",
        "  ax2.plot(epochs, training_accuracy, 'bo', label='Training Accuracy')\n",
        "  ax2.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
        "  ax2.title.set_text('Training vs. Validation Accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbSk2qE1cEfS"
      },
      "outputs": [],
      "source": [
        "plot_train_vs_val_performance(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdeEdLD6W91U"
      },
      "source": [
        "We'll initialize a new embedding layer and model and train for epochs equalling the point where we saw the validation loss diverge from the training loss.<br>\n",
        "\n",
        "**NOTE**: We need to initialize a new embedding layer here because we set the *learnable* parameter to **True** in the previous embedding layer. This means the previous embeddings were almost certainly updated by the learning algorithm. So we're re-training a new model now with the original embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_nWiYy6cEbJ"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    input_length=MAX_REVIEW_LEN,\n",
        "    trainable=True\n",
        ")\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=512, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4X_sdScYPAP"
      },
      "source": [
        "Now that we have a trained model, let's try it on the test data. As we did with the training data, we'll:\n",
        "1. Replace the labels with 0 for negative sentiment, and 1 for positive.\n",
        "2. Convert the reviews into a sequence of integers and pad/truncate each review to a fixed length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUsripS7SZdk"
      },
      "outputs": [],
      "source": [
        "yelp_test = pd.read_csv('yelp_review_polarity_csv/test.csv', names=['sentiment', 'review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm4Day_jYbTe"
      },
      "outputs": [],
      "source": [
        "yelp_test['sentiment'].replace(to_replace=1, value=0, inplace=True)\n",
        "yelp_test['sentiment'].replace(to_replace=2, value=1, inplace=True)\n",
        "yelp_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qks0uXyzYbTe"
      },
      "outputs": [],
      "source": [
        "y_test = np.array(yelp_test['sentiment'])\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UyBts6EYbTe"
      },
      "outputs": [],
      "source": [
        "X_test = tokenizer.texts_to_sequences(yelp_test['review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loh2rh3WYbTe"
      },
      "outputs": [],
      "source": [
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKmqmLUkYbTf"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GBMe-YKZCDP"
      },
      "source": [
        "Not bad for a conceptually simple model where we average out a review's word vectors, run it through a few plain hidden layers, and out through a sigmoid function with no regularization and just using defaults for model components (e.g. optimizer settings).<br><br>\n",
        "We can now use the model for predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNbNyr8PVJMv"
      },
      "outputs": [],
      "source": [
        "def sentiment(reviews):\n",
        "  seqs = tokenizer.texts_to_sequences(reviews)\n",
        "  seqs = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_REVIEW_LEN)\n",
        "  return model.predict(seqs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy-4MWV1VJRA"
      },
      "outputs": [],
      "source": [
        "# Real reviews from Google Reviews.\n",
        "pos_review = \"The best seafood joint in East Village San Diego!  Great lobster roll, great fish, great oysters, great bread, great cocktails, and such amazing service.  The atmosphere is top notch and the location is so much fun being located just a block away from Petco Park (San Diego Padres Stadium).\"\n",
        "neg_review = \"A thoroughly disappointing experience. When you book a Marriott you expect a certain standard. Albany falls way short. Room cleaning has to be booked 24 hours in advance but nobody thought to mention this at check in. The hotel is tired and needs a face-lift. The only bright light in a sea of mediocrity were the pancakes at breakfast. Sadly they weren't enough to save the experience. If you travel to Albany, then do yourself a big favour and book the Westin.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH8b-Z3GVJU3"
      },
      "outputs": [],
      "source": [
        "print(sentiment([pos_review, neg_review]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUX0AZqYbt1y"
      },
      "source": [
        "# Training New Embeddings and a Model at the Same Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ek2bnbCVJbv"
      },
      "source": [
        "For this last model, rather than using pretrained embeddings, we'll start with a **random** embedding matrix and let the model come up with its own vectors simultaneously while fitting the training data.<br><br>\n",
        "We'll also use **early stopping**, but otherwise keep everything else the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCy90PyxVJfK"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# The 'trainable' property is True by default.\n",
        "model.add(layers.Embedding(input_dim=num_tokens, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=MAX_REVIEW_LEN))\n",
        "\n",
        "\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_val, y_val), callbacks=[es_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dL0GwLUVJio"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Uy8zP-0WUI"
      },
      "source": [
        "It looks like in this case, we get comparable performance between fine-tuning pretrained vectors and training embeddings from scratch as part of the model; likely because of the nature of the data and amount of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si2Nx7NSfl1V"
      },
      "source": [
        "# Try This"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsQYls6a0zH2"
      },
      "source": [
        "In our first model, we used pretrained vectors in the **embedding layer** and set the *trainable* property to **True**, allowing the model to fine-tune the word vectors.<br><br>\n",
        "Instantiate the same model but this time, set the *trainable* property in the **embedding layer** to **False**. What happens to training performance? Does the training speed increase or decrease? What happens if you try to add some regularization like dropout?<br>\n",
        "\n",
        "Other things you can try to see what happens: reduce the number of units, use a slower learning rate, reduce the number of hidden layers, reduce vocabulary, reduce embedding dimensions, use regularization, use a shorter sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28u6ssTzflcE"
      },
      "outputs": [],
      "source": [
        "# Instantiate the embedding layer.\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Add layers.\n",
        "\n",
        "\n",
        "# Compile model.\n",
        "\n",
        "\n",
        "# Call fit.\n",
        "\n",
        "\n",
        "# Evaluate the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgJM0Flw6Imq"
      },
      "source": [
        "# Alternative Static Embedding Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcq27-hYKdMh"
      },
      "source": [
        "## GloVe\n",
        "**GloVe (Global Vectors for Word Representation)** is another algorithm for creating static word vectors. You can read the original GloVe paper and download pretrained word vectors here:<br>\n",
        "https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "331buv_FK4nQ"
      },
      "source": [
        "## Doc2Vec\n",
        "An algorithm which represents a document as a dense vector which addresses weaknesses of bag-of-words models.<br>\n",
        "https://arxiv.org/abs/1405.4053<br>\n",
        "https://radimrehurek.com/gensim/models/doc2vec.html<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRq8HklIg2Gu"
      },
      "source": [
        "## fastText\n",
        "An alternative approach to creating embeddings. Instead of assigning a vector to each _word_ (e.g. a separate vector each for \"dog\" and \"dogs\"), a vector is assigned to each _subword_. For fastText, a subword is defined as a character n-gram.\n",
        "<br><br>\n",
        "So if n=3, then a word like \"hello\" would result in vectors for \"<he\", \"hel\", \"ell\", \"llo\", \"lo>\" (note that \"<\" and \">\" are special characters). The vector for \"hello\" would be the sum of all the above vectors. This helps deal with OOV situations because vectors can still be assigned to unseen words as long as the n-grams exist in the vocabulary.<br>\n",
        "https://fasttext.cc/<br>\n",
        "https://radimrehurek.com/gensim/models/fasttext.html\n",
        "<br><br>\n",
        "**We'll cover subword tokenization in greater detail later in the course.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "nlpdemystified-word-vectors.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
